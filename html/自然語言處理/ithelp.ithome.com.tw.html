<html>
 <body>
  <div>
   <div class="markdown__style">
    <h2>
     前言
    </h2>
    <p>
     之前我們看到 Neural Network 在影像的辨識與解析的強大威力，接著，我們就要開始研究『自然語言處理』(Natural Language Processing, NLP)，它包括文字/語音的辨識、解析與生成，實際應用範疇很廣泛，請參閱
     <a href="https://zh.wikipedia.org/wiki/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86" target="_blank">
      自然語言處理
     </a>
     一文，節錄如下：
    </p>
    <ol>
     <li>
      文字朗讀（Text to speech）/ 語音合成（Speech synthesis）
     </li>
     <li>
      語音識別（Speech recognition）
     </li>
     <li>
      自動分詞（word segmentation）
     </li>
     <li>
      詞性標註（Part-of-speech tagging）
     </li>
     <li>
      句法分析（Parsing）
     </li>
     <li>
      自然語言生成（Natural language generation）
     </li>
     <li>
      文字分類（Text categorization）
     </li>
     <li>
      資訊檢索（Information retrieval）
     </li>
     <li>
      資訊抽取（Information extraction）
     </li>
     <li>
      文字校對（Text-proofing）
     </li>
     <li>
      問答系統（Question answering）
     </li>
     <li>
      機器翻譯（Machine translation）
     </li>
     <li>
      自動摘要（Automatic summarization）
     </li>
     <li>
      文字蘊涵（Textual entailment）
     </li>
    </ol>
    <p>
     我們先來觀察一個典型的『聊天機器人』(ChatBot)流程，如下圖：
     <br/>
     <img alt="https://ithelp.ithome.com.tw/upload/images/20171209/20001976yRr8vw6sfj.jpg" src="https://ithelp.ithome.com.tw/upload/images/20171209/20001976yRr8vw6sfj.jpg"/>
     <br/>
     圖. 聊天機器人(ChatBot)文字/語音的解析及處理
    </p>
    <p>
     上圖場景涵蓋的相關技術如下：
    </p>
    <ol>
     <li>
      當人對機器說了一句話，機器要先把那句話轉成文字，稱之為『語音識別』(Speech To Text or Speech recognition)。
     </li>
     <li>
      機器對那段文字做解析，瞭解那段文字代表甚麼意義，或者該回甚麼話，稱之為『Text Understanding』。
     </li>
     <li>
      機器回覆有兩種方式：
      <ul>
       <li>
        以文字回覆：必須自詞庫找出要回覆的一段文字，稱之為『Text Generation』。
       </li>
       <li>
        以語言回覆：將文字轉為語音合成，稱之為『Text To Speech』。
       </li>
      </ul>
     </li>
    </ol>
    <p>
     其中『 文字朗讀』(Text To Speech)技術已非常成熟，Windows平臺提供 Microsoft Speech API (SAPI)，Android/iOS 同樣也提供現成的函式可呼叫，其他『Text Understanding』、『Text Generation』、『Speech To Text』則有賴於AI技術的後援。
    </p>
    <h2>
     語文的特性
    </h2>
    <p>
     不像影像相對於點陣圖(Bitmap)的單純，人類的語言具高度曖昧性，一句話可能有多重的意思或隱喻，且隨著時代演進不斷變化，人們不斷創造新詞，語言學家越來越難建立規則(文法及語意學)來規範語文。另外，由於語文的不規則性，在放入模型解析之前，我們通常會先對輸入的語文做前置處理(Preprocess)，包括：
    </p>
    <ol>
     <li>
      <p>
       資料清理(Data Cleaning)：例如我們抓網頁資料，必須先清除 HTML 標籤(Tag)，取出乾淨的本文。
      </p>
     </li>
     <li>
      <p>
       標點符號(Punctuation)：對語意沒有影響，通常會忽略他們。
      </p>
     </li>
     <li>
      <p>
       分詞(Tokenize)：英文較容易，一般以空白即可，但中文就比較困難。
      </p>
     </li>
     <li>
      <p>
       Stop Words：例如英文的『the』、『as』、『to』、『from』...等介係詞或助詞，他們對語文大意的瞭解可能沒有太大的幫助，通常會忽略他們。
      </p>
     </li>
     <li>
      <p>
       『詞嵌入』(Word Embedding)：要交給電腦計算，將單字轉數值會比較容易處理，『詞嵌入』技術通常會將單字轉為實數，以形成連續的向量空間，比較有名的模型包括
       <a href="https://code.google.com/archive/p/word2vec/" target="_blank">
        Word2Vec
       </a>
       及
       <a href="https://nlp.stanford.edu/projects/glove/" target="_blank">
        GloVe
       </a>
       。
      </p>
     </li>
     <li>
      <p>
       相似詞整理：意義相近的單字或片語，在解析字句及分類時必須能呈現出來，Google Tomas Mikolov 創造 Word2Vec 模型，以向量(Vector)空間來定義單字(Word)，就如之前介紹的『
       <a href="https://ithelp.ithome.com.tw/articles/10192389" target="_blank">
        照片比對
       </a>
       』計算Cosine，如接近1，就表示兩單字的意義相似。
       <br/>
       <img alt="https://ithelp.ithome.com.tw/upload/images/20171222/20001976mGA850p1Vh.jpg" src="https://ithelp.ithome.com.tw/upload/images/20171222/20001976mGA850p1Vh.jpg"/>
       <br/>
       圖. Word2vec向量空間，圖片來源：
       <a href="https://opensource.googleblog.com/2013/08/learning-meaning-behind-words.html" target="_blank">
        Google Open Source Blog
       </a>
      </p>
     </li>
     <li>
      <p>
       『語料庫』(Corpus)：要能訓練模型，必須要有大量的標註資料，
       <a href="http://www.nltk.org/" target="_blank">
        NLTK
       </a>
       (Natural Language Toolkit)工具箱同時提供完整的函式庫及大量的
       <a href="http://www.nltk.org/nltk_data/" target="_blank">
        語料庫
       </a>
       ，各式的語料庫可透過下列指令下載：
      </p>
     </li>
     <li>
      <p>
       安裝 NLTK 函式庫
       <br/>
       pip install -U nltk
      </p>
     </li>
     <li>
      <p>
       在DOS內輸入Python
       <br/>
       import nltk
       <br/>
       nltk.download()
       <br/>
       或直接下載全部語料庫:
       <br/>
       python -m nltk.downloader all
      </p>
     </li>
    </ol>
    <h2>
     前置處理(Preprocess)
    </h2>
    <p>
     我們就來看幾個前置處理的程式碼。
    </p>
    <ol>
     <li>
      去除 HTML 標籤(Tag)
      <br/>
      NLTK函式庫不再提供此功能，建議使用 BeautifulSoup 函式庫，請依下列指令安裝BeautifulSoup：
      <br/>
      pip install BeautifulSoup4
      <br/>
      測試如下：
     </li>
    </ol>
    <pre><code>from bs4 import BeautifulSoup
html='&lt;h2 class="block-title"&gt;今日最新&lt;/h2&gt;'
soup = BeautifulSoup(html, 'lxml')
soup.get_text() # 得到結果為 '今日最新'
</code></pre>
    <ol start="2">
     <li>
      分詞(Tokenize)：
     </li>
    </ol>
    <pre><code>from nltk.tokenize import word_tokenize
# 測試字句
sent = "the the the dog, dog some other words that we do not care about"
# 取出每個單字
list=[word for word in word_tokenize(sent)]
#得到結果為 ['the', 'the', 'the', 'dog', ',', 'dog', 'some', 'other', 'words', 'that', 'we', 'do', 'not', 'care', 'about']
# 去除重複，並排序
vacabulary = sorted(set(list)) 
#得到結果為 [',', 'about', 'care', 'do', 'dog', 'not', 'other', 'some', 'that', 'the', 'we', 'words']
# 求得每個單字的出現頻率
import nltk
freq = nltk.FreqDist(list)
#得到結果為 FreqDist({'the': 3, 'dog': 2, 'care': 1, 'some': 1, 'other': 1, ',': 1, 'we': 1, 'that': 1, 'words': 1, 'about': 1, ...})

# 作圖
freq.plot()
</code></pre>
    <p>
     <img alt="https://ithelp.ithome.com.tw/upload/images/20171223/20001976VsOL24i45b.png" src="https://ithelp.ithome.com.tw/upload/images/20171223/20001976VsOL24i45b.png"/>
    </p>
    <ol start="3">
     <li>
      Stop Words 處理：
     </li>
    </ol>
    <pre><code>stopwords=[",","the"]
# 去除 Stop Words
list=[word for word in word_tokenize(sent) if word not in stopwords]
#得到結果為 ['dog', 'dog', 'some', 'other', 'words', 'that', 'we', 'do', 'not', 'care', 'about']
</code></pre>
    <ol start="4">
     <li>
      『詞嵌入』(Word Embedding)： Keras 提供 Embedding 層，可直接轉換。
     </li>
     <li>
      另外，針對英文及一些語系，動詞還有分原形、過去式、進行式、過去分詞...等，名詞有單、複數，形容詞有比較級、最高階，在作語文處理時，會希望統一改為原形，避免為每個單字要分別建立多個key，這種轉換，稱之為『詞形還原』(Lemmatization)。
     </li>
    </ol>
    <pre><code># 記得載入 WordNet 語料庫
from nltk.stem import WordNetLemmatizer
wnl = WordNetLemmatizer()
# 要指定單字詞性(pos)
print(wnl.lemmatize('ate', pos='v')) # 得到 eat
print(wnl.lemmatize('better', pos='a')) # 得到 good
print(wnl.lemmatize('dogs')) # 得到 dog
# 若要自動取得單字詞性(pos)，請參考 http://www.zmonster.me/2016/01/21/lemmatization-survey.html。
</code></pre>
    <p>
     雖然，利用語料庫很方便，但是還是會有點問題，例如：
    </p>
    <ol>
     <li>
      <p>
       'I saw two bigger dogs than this one.'
       <br/>
       會變成
       <br/>
       ['I', 'saw', 'two', 'big', 'dog', 'than', 'this', 'one', '.']
       <br/>
       <strong>
        saw 不會還原，因 saw 另一個意思是鋸的原形
       </strong>
      </p>
     </li>
     <li>
      <p>
       'I ate two bigger cookies than this one.'
       <br/>
       會變成
       <br/>
       ['I', 'eat', 'two', 'big', 'cooky', 'than', 'this', 'one', '.']
       <br/>
       <strong>
        cookies 會還原為 cooky，而不是我們熟知的 cookie
       </strong>
      </p>
     </li>
    </ol>
    <h2>
     『自然語言處理』(NLP) 演演算法
    </h2>
    <p>
     要理解一段話，首先要從單字開始，進而到片語(Phrase)、句子(Sentence)，加上文法(Syntax)、語意(Semantics)解析，我們才能理解這一段話的意義，如果要從這樣的導向(Approach)出發，我們可能要聘請語言學專家建置龐大的『規則引擎』(Rule Engine)才行，工程浩大，所費不貲，維護成本可能也是天文數字，反之，從另一種角度思考，可否與CNN一樣，改為提供大量字句，讓機器自我學習 ?
     <br/>
     如果，我們把字句當作 input，餵入 Neural Network 模型，希望機器產生適當的回應，會有幾個問題要解決：
    </p>
    <ol>
     <li>
      字句有長有短，即 input 變數數目不等，回應也是一樣的狀況，output 變數數目也不等。
     </li>
     <li>
      語文會有上下文的關係，斷章取義(政治媒體的專長?)常會扭曲全文代表的意義。
     </li>
     <li>
      人類擁有記憶力，對某些人、事、物會有深度的連結，講到『川普』，可能會想到『北韓』，若機器擁有記憶力機制，預測準確率就會高很多。
     </li>
    </ol>
    <p>
     針對以上的語文特性，學者提出三個解決問題的演演算法，分別為『迴圈神經網路』(Recurrent Neural Network, RNN)、長短期記憶網路(Long Short Term Memory Network, LSTM)及 GRU (Gated Recurrent Unit，查不到中文譯名)，下一篇我們就一一來瞭解它們，並撰寫程式以及應用場域說明。
    </p>
   </div>
  </div>
 </body>
</html>